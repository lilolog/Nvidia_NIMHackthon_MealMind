import gradio as gr
import time
from openai import OpenAI
import os
from dotenv import load_dotenv
# Load environment variables from .env file
load_dotenv()

# å®šä¹‰è«å…°è¿ªæ˜¥å­£è‰²å½©ä¸»é¢˜
morandi_colors = {
    "mint_green": "#C7D3CC",  # ä¸»è‰²è°ƒï¼šæ·¡è–„è·ç»¿
    "soft_pink": "#E8D6CF",   # è¾…åŠ©è‰²1ï¼šæŸ”å’Œç²‰è‰²
    "light_blue": "#B8C5D6",  # è¾…åŠ©è‰²2ï¼šæ·¡è“è‰²
    "warm_beige": "#E4CFBF",  # è¾…åŠ©è‰²3ï¼šæµ…æè‰²
    "background": "#F5F2ED",  # èƒŒæ™¯è‰²ï¼šç±³ç™½è‰²
    "text": "#5D5C58"         # æ–‡å­—è‰²ï¼šæ·±ç°è¤è‰²
}

# è‡ªå®šä¹‰CSSæ ·å¼ï¼Œä½¿ç”¨è«å…°è¿ªè‰²å½©
custom_css = """
:root {
    --mint-green: """ + morandi_colors["mint_green"] + """;
    --soft-pink: """ + morandi_colors["soft_pink"] + """;
    --light-blue: """ + morandi_colors["light_blue"] + """;
    --warm-beige: """ + morandi_colors["warm_beige"] + """;
    --background: """ + morandi_colors["background"] + """;
    --text: """ + morandi_colors["text"] + """;
}

body {
    background-color: var(--background);
    color: var(--text);
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.gradio-container {
    background-color: var(--background);
}

/* æ ‡é¢˜æ ·å¼ */
h1 {
    color: var(--text);
    font-size: 2.5rem;
    text-align: center;
    margin-bottom: 1.5rem;
    padding: 1rem;
    background: linear-gradient(to right, var(--mint-green), var(--soft-pink));
    border-radius: 10px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

h2 {
    color: var(--text);
    font-size: 1.8rem;
    border-bottom: 2px solid var(--mint-green);
    padding-bottom: 0.5rem;
    margin-bottom: 1rem;
}

/* æ ‡ç­¾é¡µæ ·å¼ */
.tabs {
    background-color: var(--background);
    border-radius: 10px;
    overflow: hidden;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
}

.tab-nav {
    background-color: var(--mint-green);
    padding: 0.5rem;
    border-radius: 10px 10px 0 0;
}

.tab-nav button {
    background-color: transparent;
    color: var(--text);
    border: none;
    padding: 0.8rem 1.5rem;
    margin: 0 0.2rem;
    border-radius: 8px 8px 0 0;
    font-weight: 600;
    transition: all 0.3s ease;
}

.tab-nav button:hover, .tab-nav button.selected {
    background-color: var(--background);
    color: var(--text);
}

/* èŠå¤©ç•Œé¢æ ·å¼ */
.message-wrap {
    background-color: var(--background);
    border-radius: 10px;
    padding: 1rem;
    margin-bottom: 1rem;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
}

.user-message {
    background-color: var(--light-blue);
    color: var(--text);
    border-radius: 18px 18px 0 18px;
    padding: 1rem;
    margin: 0.5rem 0;
}

.bot-message {
    background-color: var(--soft-pink);
    color: var(--text);
    border-radius: 18px 18px 18px 0;
    padding: 1rem;
    margin: 0.5rem 0;
}

/* è¾“å…¥æ¡†å’ŒæŒ‰é’®æ ·å¼ */
input[type="text"], textarea {
    background-color: white;
    border: 2px solid var(--mint-green);
    border-radius: 8px;
    padding: 0.8rem;
    color: var(--text);
    font-size: 1rem;
}

button {
    background-color: var(--mint-green);
    color: var(--text);
    border: none;
    border-radius: 8px;
    padding: 0.8rem 1.5rem;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
}

button:hover {
    background-color: var(--soft-pink);
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
}

/* ç§»é™¤äº†æ˜¥å­£è£…é¥°å…ƒç´  */
"""

# åˆ›å»ºè‡ªå®šä¹‰ä¸»é¢˜ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
theme = gr.themes.Base(
    primary_hue="green",
    secondary_hue="pink",
    neutral_hue="gray",
    font=("Segoe UI", "ui-sans-serif", "system-ui", "sans-serif"),
)

# è®¾ç½®è‡ªå®šä¹‰é¢œè‰²
theme.background_fill = morandi_colors["background"]
theme.button_primary_background_fill = morandi_colors["mint_green"]
theme.button_primary_background_fill_hover = morandi_colors["soft_pink"]
theme.button_secondary_background_fill = morandi_colors["light_blue"]
theme.button_secondary_background_fill_hover = morandi_colors["warm_beige"]
theme.block_title_text_color = morandi_colors["text"]
theme.block_label_text_color = morandi_colors["text"]
theme.input_background_fill = "white"
theme.input_border_color = morandi_colors["mint_green"]

NVIDIA_API_KEY = os.getenv('NVIDIA_API_KEY')
NVIDIA_MODEL = os.getenv('NV_MODEL')

def generate_recommended_response(message, history):
    """
    Function to stream OpenAI responses for é¥®é£Ÿé¢„ç®— tab.
    """
    client = OpenAI(
        base_url = "https://integrate.api.nvidia.com/v1",
        api_key = "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"
    )
    # Prepare the messages in the required format
    messages = [{"role": "system", "content": "ä½ æ˜¯ç”Ÿé²œè¶…å¸‚çš„é”€å”®å‘˜,åº—é‡Œæœ€è¿‘æ¥äº†ä¸€æ‰¹æ—¶ä»¤æ˜¥èœ,æœ‰è èœ,æ˜¥ç¬‹,è•¨èœ,è¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„é‡‡è´­éœ€æ±‚,æ ¹æ®è¥å…»å­¦æ­é…åŸåˆ™,æ¨èå®¢æˆ·è´­ä¹°æ–°çš„æ˜¥èœ,åŒæ—¶è¦å¼•å¯¼å®¢æˆ·è´­ä¹°åº—é‡Œå…¶ä»–çš„å•†å“,æ¯”å¦‚é¸¡è›‹,ç‰›å¥¶,é¢åŒ…ç­‰,å¹¶ä¸”è¦ç»™å‡ºåˆç†çš„æ­é…æ–¹æ¡ˆ,è®©å®¢æˆ·è§‰å¾—ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¥å…»å¸ˆ.åœ¨å¯¹è¯æœ«å°¾è¦åˆ—å‡ºéœ€è¦é‡‡è´­çš„é£Ÿæ,å¹¶ä»¥markdownæ ¼å¼è¾“å‡ºè¡¨æ ¼,åŒæ—¶æç¤ºç”¨æˆ·å»è¶…å¸‚é‡Œç°åœºä½“éªŒè´­ä¹°,è¯·æ ¹æ®ç”¨æˆ·çš„æé—®è¿›è¡Œå›ç­”,æ–‡å­—æœ«å°¾åŠ å…¥å¦‚ä¸‹å†…å®¹:ä½ è¦æ˜¯å»é€›çš„è¯,æˆ‘å¸®ä½ æ‰“å°ä¸€ä»½æ¸…å•"}]
    for msg in history:
        if 'role' in msg:
            if msg['role'] == 'user':
                messages.append({"role": "user", "content": msg['content']})
            if msg['role'] == 'assistant':
                messages.append({"role": "assistant", "content": msg['content']})
    # Add the current user message to the conversation history if it's not empty
    if message and message.strip():
        messages.append({"role": "user", "content": message})
      # Call OpenAI's API with streaming enabled
    # meta/llama-3.3-70b-instruct
    completion = client.chat.completions.create(
        model="deepseek-ai/deepseek-r1-distill-qwen-32b",
        messages=messages,
        temperature=0.6,
        top_p=0.7,
        max_tokens=4096,
        stream=True
    )
    # Stream the response chunks
    bot_message = ""
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            bot_message += chunk.choices[0].delta.content
            yield history + [{"role": "assistant", "content": bot_message}]

def generate_emotional_recipe_response(message, history):
    """
    Function to stream OpenAI responses for æƒ…æ„Ÿèœè°± tab.
    """
    client = OpenAI(
        base_url = "https://integrate.api.nvidia.com/v1",
        api_key = "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"
    )
    # Prepare the messages in the required format
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æ„Ÿç¾é£Ÿé¡¾é—®ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªçŠ¶æ€æ¨èé€‚åˆçš„é£Ÿè°±ã€‚ä½ æ“…é•¿åˆ†æç”¨æˆ·çš„å¿ƒæƒ…ï¼Œå¹¶æ¨èèƒ½å¤Ÿæ”¹å–„æƒ…ç»ªæˆ–é…åˆæƒ…ç»ªçš„ç¾é£Ÿã€‚è¯·æ ¹æ®ç”¨æˆ·æè¿°çš„æƒ…æ„ŸçŠ¶æ€ï¼Œæ¨èé€‚åˆçš„é£Ÿè°±ï¼Œå¹¶è§£é‡Šè¿™äº›é£Ÿç‰©å¦‚ä½•å½±å“æƒ…ç»ªã€‚åœ¨å›ç­”ä¸­ï¼Œè¯·æä¾›è¯¦ç»†çš„çƒ¹é¥ªæ­¥éª¤ï¼Œå¹¶ä»¥markdownæ ¼å¼è¾“å‡ºé£Ÿææ¸…å•ã€‚"}]
    for msg in history:
        if 'role' in msg:
            if msg['role'] == 'user':
                messages.append({"role": "user", "content": msg['content']})
            if msg['role'] == 'assistant':
                messages.append({"role": "assistant", "content": msg['content']})
    # Add the current user message to the conversation history if it's not empty
    if message and message.strip():
        messages.append({"role": "user", "content": message})
    # Call OpenAI's API with streaming enabled
    completion = client.chat.completions.create(
        model="deepseek-ai/deepseek-r1-distill-qwen-32b",
        messages=messages,
        temperature=0.6,
        top_p=0.7,
        max_tokens=4096,
        stream=True
    )
    # Stream the response chunks
    bot_message = ""
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            bot_message += chunk.choices[0].delta.content
            yield history + [{"role": "assistant", "content": bot_message}]

def generate_random_food_response(message, history):
    """
    Function to stream OpenAI responses for éšæœºç¾é£Ÿ tab.
    """
    client = OpenAI(
        base_url = "https://integrate.api.nvidia.com/v1",
        api_key = "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"
    )
    # Prepare the messages in the required format
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½åˆ›æ„ç¾é£Ÿä¸“å®¶ï¼Œæ“…é•¿æ¨èæ–°é¢–ã€æ„æƒ³ä¸åˆ°çš„é£Ÿç‰©ç»„åˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„å–œå¥½æˆ–è¦æ±‚ï¼Œéšæœºæ¨èæœ‰è¶£ä¸”ç¾å‘³çš„é£Ÿç‰©æ­é…ã€‚è¿™äº›æ¨èå¯ä»¥æ‰“ç ´ä¼ ç»Ÿï¼Œä½†å¿…é¡»ä¿è¯ç¾å‘³å’Œå¯è¡Œæ€§ã€‚åœ¨å›ç­”ä¸­ï¼Œè¯·æä¾›è¯¦ç»†çš„é£Ÿææ¸…å•å’Œç®€å•çš„åˆ¶ä½œæ–¹æ³•ï¼Œå¹¶ä»¥markdownæ ¼å¼å‘ˆç°ã€‚æ¯æ¬¡å›ç­”éƒ½åº”è¯¥åŒ…å«ä¸€äº›æƒŠå–œå…ƒç´ ï¼Œè®©ç”¨æˆ·æ„Ÿåˆ°æ–°é²œå’Œå…´å¥‹ã€‚"}]
    for msg in history:
        if 'role' in msg:
            if msg['role'] == 'user':
                messages.append({"role": "user", "content": msg['content']})
            if msg['role'] == 'assistant':
                messages.append({"role": "assistant", "content": msg['content']})
    # Add the current user message to the conversation history if it's not empty
    if message and message.strip():
        messages.append({"role": "user", "content": message})
    # Call OpenAI's API with streaming enabled
    completion = client.chat.completions.create(
        model="deepseek-ai/deepseek-r1-distill-qwen-32b",
        messages=messages,
        temperature=0.6,
        top_p=0.7,
        max_tokens=4096,
        stream=True
    )
    # Stream the response chunks
    bot_message = ""
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            bot_message += chunk.choices[0].delta.content
            yield history + [{"role": "assistant", "content": bot_message}]

def generate_nutrition_response(message, history):
    """
    Function to stream OpenAI responses for è¥å…»å®šåˆ¶ tab.
    """
    client = OpenAI(
        base_url = "https://integrate.api.nvidia.com/v1",
        api_key = "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"
    )
    # Prepare the messages in the required format
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¥å…»å¸ˆï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„å¥åº·çŠ¶å†µå’Œç›®æ ‡æä¾›ä¸ªæ€§åŒ–çš„é¥®é£Ÿå»ºè®®ã€‚ä½ æ“…é•¿åˆ†æç”¨æˆ·çš„è¥å…»éœ€æ±‚ï¼Œå¹¶è®¾è®¡ç¬¦åˆè¿™äº›éœ€æ±‚çš„é¥®é£Ÿè®¡åˆ’ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯ï¼ˆå¦‚å¥åº·ç›®æ ‡ã€é¥®é£Ÿé™åˆ¶ã€è¿‡æ•åŸç­‰ï¼‰ï¼Œæä¾›è¯¦ç»†çš„è¥å…»å»ºè®®å’Œé¥®é£Ÿè®¡åˆ’ã€‚åœ¨å›ç­”ä¸­ï¼Œè¯·è§£é‡Šå„ç§é£Ÿç‰©çš„è¥å…»ä»·å€¼ï¼Œå¹¶ä»¥markdownæ ¼å¼æä¾›æ¯æ—¥æˆ–æ¯å‘¨çš„é¥®é£Ÿå®‰æ’è¡¨ã€‚"}]
    for msg in history:
        if 'role' in msg:
            if msg['role'] == 'user':
                messages.append({"role": "user", "content": msg['content']})
            if msg['role'] == 'assistant':
                messages.append({"role": "assistant", "content": msg['content']})
    # Add the current user message to the conversation history if it's not empty
    if message and message.strip():
        messages.append({"role": "user", "content": message})
    # Call OpenAI's API with streaming enabled
    completion = client.chat.completions.create(
        model="deepseek-ai/deepseek-r1-distill-qwen-32b",
        messages=messages,
        temperature=0.6,
        top_p=0.7,
        max_tokens=4096,
        stream=True
    )
    # Stream the response chunks
    bot_message = ""
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            bot_message += chunk.choices[0].delta.content
            yield history + [{"role": "assistant", "content": bot_message}]

def user_input(user_message, history):
    """
    Function to handle user input for all tabs.
    """
    return "", history + [{"role": "user", "content": user_message}]

# ä½¿ç”¨ Blocks API åˆ›å»ºå¤šæ ‡ç­¾é¡µç•Œé¢
with gr.Blocks(css=custom_css, theme=theme) as demo:
# åº”ç”¨æ ‡é¢˜
    gr.Markdown("""
    ##  ğŸ¥—MealMindé¥­è°‹å¼•æ“ğŸ¥¬ 
    *æ˜¥é£é€æš–ï¼Œç¾é£Ÿç›¸ä¼´*
    """)

    with gr.Tabs():
        # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šé¥®é£Ÿé¢„ç®—
        with gr.Tab("é¢„ç®—"):
            gr.Markdown("""
            ### SmartFoodPlanæ™ºèƒ½é¥®é£Ÿé¢„ç®—ç®¡å®¶
            """)
            budget_chatbot = gr.Chatbot(type="messages")
            budget_msg = gr.Textbox(label="Your Message")
            budget_clear = gr.Button("Clear Chat")
            
            # Link the user input and bot response functions
            budget_msg.submit(user_input, [budget_msg, budget_chatbot], [budget_msg, budget_chatbot], queue=False).then(
                generate_recommended_response, inputs=[budget_msg, budget_chatbot], outputs=budget_chatbot
            )
            budget_clear.click(lambda: None, None, budget_chatbot, queue=False)
        
        # ç¬¬äºŒä¸ªæ ‡ç­¾é¡µï¼šæƒ…æ„Ÿèœè°±
        with gr.Tab("æƒ…æ„Ÿ"):
            gr.Markdown("""
            ### FeelTasteæƒ…æ„Ÿé©±åŠ¨çš„èœè°±è®¾è®¡å¸ˆ
            """)
            emotion_chatbot = gr.Chatbot(type="messages")
            emotion_msg = gr.Textbox(label="Your Message")
            emotion_clear = gr.Button("Clear Chat")
            
            # Link the user input and bot response functions
            emotion_msg.submit(user_input, [emotion_msg, emotion_chatbot], [emotion_msg, emotion_chatbot], queue=False).then(
                generate_emotional_recipe_response, inputs=[emotion_msg, emotion_chatbot], outputs=emotion_chatbot
            )
            emotion_clear.click(lambda: None, None, emotion_chatbot, queue=False)

        # ç¬¬ä¸‰ä¸ªæ ‡ç­¾é¡µï¼šéšæœºç¾é£Ÿ
        with gr.Tab("éšæœº"):
            gr.Markdown("""
            ### ChoiceLessè½»å†³ç­–éšæœºç¾é£ŸåŠ©æ‰‹
            """)
            random_chatbot = gr.Chatbot(type="messages")
            random_msg = gr.Textbox(label="Your Message")
            random_clear = gr.Button("Clear Chat")
            
            # Link the user input and bot response functions
            random_msg.submit(user_input, [random_msg, random_chatbot], [random_msg, random_chatbot], queue=False).then(
                generate_random_food_response, inputs=[random_msg, random_chatbot], outputs=random_chatbot
            )
            random_clear.click(lambda: None, None, random_chatbot, queue=False)
            
        # ç¬¬å››ä¸ªæ ‡ç­¾é¡µï¼šè¥å…»å®šåˆ¶
        with gr.Tab("è¥å…»"):
            gr.Markdown("""
            ### NutriGoalè¥å…»ç›®æ ‡åå‘å®šåˆ¶ç³»ç»Ÿ
            """)
            nutrition_chatbot = gr.Chatbot(type="messages")
            nutrition_msg = gr.Textbox(label="Your Message")
            nutrition_clear = gr.Button("Clear Chat")
            
            # Link the user input and bot response functions
            nutrition_msg.submit(user_input, [nutrition_msg, nutrition_chatbot], [nutrition_msg, nutrition_chatbot], queue=False).then(
                generate_nutrition_response, inputs=[nutrition_msg, nutrition_chatbot], outputs=nutrition_chatbot
            )
            nutrition_clear.click(lambda: None, None, nutrition_chatbot, queue=False)

# å¯åŠ¨åº”ç”¨
demo.launch(share=False, server_name='0.0.0.0')
